logs of local_proxy_server when Jan accepts ollama responses but not gemini:

PS C:\Users\jperr\Documents\GitHub\Zeroconf-AI> python .\local_proxy_client.py
==================================================
  ZeroconfAI Local Proxy
==================================================
Starting proxy on 127.0.0.1:8082
Configure Jan to connect to: http://127.0.0.1:8082/v1
To configure:Open Jan Setting -> Model Providers -> Add Provider -> Any name -> Api Key = Any string -> Base URL = http://127.0.0.1:8082/v1

[15:51:41] [DEBUG] Using proactor: IocpProactor
[15:51:41] [INFO] Waiting for service discovery...
[15:51:42] [INFO] Discovered service: Gemini._zeroconfai._tcp.local. at 192.168.56.1:8081 (priority: 51)
[15:51:42] [INFO] Discovered service: Ollama._zeroconfai._tcp.local. at 192.168.56.1:8080 (priority: 50)
[15:51:44] [DEBUG] Using proactor: IocpProactor
INFO:     Started server process [37544]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8082 (Press CTRL+C to quit)
[15:51:51] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:51:51] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:51:51] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:51:51] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:51:51] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:51:51] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:51:51] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:51:51] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:51:58] [INFO] === NEW REQUEST ===
[15:51:58] [INFO] Model: Gemini-2.5-Flash
[15:51:58] [INFO] Messages: 2
[15:51:58] [INFO] Stream: True
[15:51:58] [INFO] Max tokens: None
[15:51:58] [DEBUG] Message 0: system - You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks
[15:51:58] [DEBUG] Message 1: user - hello
[15:51:58] [INFO] Request streaming mode: True
[15:51:58] [DEBUG] Full request data: {
  "model": "Gemini-2.5-Flash",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks to the best of your abilities.\n\nWhen responding:\n- Answer directly from your knowledge when you can\n- Be concise, clear, and helpful\n- Admit when you\u2019re unsure rather than making things up\n\nIf tools are available to you:\n- Only use tools when they add real value to your response\n- Use tools when the user explicitly asks (e.g., \"search for...\", \"calculate...\", \"run this code\")\n- Use tools for information you don\u2019t know or that needs verification\n- Never use tools just because they\u2019re available\n\nWhen using tools:\n- Use one tool at a time and wait for results\n- Use actual values as arguments, not variable names\n- Learn from each result before deciding next steps\n- Avoid repeating the same tool call with identical parameters\n\nRemember: Most questions can be answered without tools. Think first whether you need them.\n\nCurrent date: November 4, 2025"
    },
    {
      "role": "user",
      "content": "hello"
    }
  ],
  "stream": true
}
[15:51:58] [INFO] Routing 'Gemini-2.5-Flash' request to Gemini._zeroconfai._tcp.local. at http://192.168.56.1:8081
[15:51:58] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:51:58] [DEBUG] http://192.168.56.1:8081 "POST /v1/chat/completions HTTP/1.1" 200 None
[15:51:58] [INFO] Response status: 200
[15:51:58] [DEBUG] Response headers: {'date': 'Tue, 04 Nov 2025 23:51:57 GMT', 'server': 'uvicorn', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'x-accel-buffering': 'no', 'content-type': 'text/event-stream; charset=utf-8', 'Transfer-Encoding': 'chunked'}
[15:51:58] [INFO] Returning streaming response from Gemini._zeroconfai._tcp.local.
[15:51:58] [INFO] Setting up streaming response
INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[15:51:58] [INFO] Stream complete. Total chunks: 0
[15:52:01] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:01] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:52:01] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:01] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:52:01] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:01] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:52:01] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:01] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:52:11] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:11] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:52:11] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:11] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:52:11] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:11] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:52:11] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:11] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:52:21] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:21] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:52:21] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:21] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:52:21] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:21] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:52:21] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:21] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:52:31] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:31] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:52:31] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:31] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:52:31] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:31] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:52:31] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:31] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:52:41] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:41] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:52:41] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:42] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:52:42] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:42] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:52:42] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:42] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:52:52] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:52] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:52:52] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:52:52] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:52:52] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:52] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:52:52] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:52:52] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:52:54] [INFO] === NEW REQUEST ===
[15:52:54] [INFO] Model: qwen3:4b-instruct
[15:52:54] [INFO] Messages: 4
[15:52:54] [INFO] Stream: True
[15:52:54] [INFO] Max tokens: None
[15:52:54] [DEBUG] Message 0: system - You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks
[15:52:54] [DEBUG] Message 1: user - hello
[15:52:54] [DEBUG] Message 2: assistant - .
[15:52:54] [DEBUG] Message 3: user - hi
[15:52:54] [INFO] Request streaming mode: True
[15:52:54] [DEBUG] Full request data: {
  "model": "qwen3:4b-instruct",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks to the best of your abilities.\n\nWhen responding:\n- Answer directly from your knowledge when you can\n- Be concise, clear, and helpful\n- Admit when you\u2019re unsure rather than making things up\n\nIf tools are available to you:\n- Only use tools when they add real value to your response\n- Use tools when the user explicitly asks (e.g., \"search for...\", \"calculate...\", \"run this code\")\n- Use tools for information you don\u2019t know or that needs verification\n- Never use tools just because they\u2019re available\n\nWhen using tools:\n- Use one tool at a time and wait for results\n- Use actual values as arguments, not variable names\n- Learn from each result before deciding next steps\n- Avoid repeating the same tool call with identical parameters\n\nRemember: Most questions can be answered without tools. Think first whether you need them.\n\nCurrent date: November 4, 2025"
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "."
    },
    {
      "role": "user",
      "content": "hi"
    }
  ],
  "stream": true
}
[15:52:54] [INFO] Routing 'qwen3:4b-instruct' request to Ollama._zeroconfai._tcp.local. at http://192.168.56.1:8080
[15:52:54] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:02] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:02] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:53:02] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:02] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:53:02] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:05] [INFO] Ollama._zeroconfai._tcp.local. is now unhealthy
[15:53:06] [DEBUG] http://192.168.56.1:8080 "POST /v1/chat/completions HTTP/1.1" 200 None
[15:53:06] [INFO] Response status: 200
[15:53:06] [DEBUG] Response headers: {'date': 'Tue, 04 Nov 2025 23:52:53 GMT', 'server': 'uvicorn', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'x-accel-buffering': 'no', 'content-type': 'text/event-stream; charset=utf-8', 'Transfer-Encoding': 'chunked'}
[15:53:06] [INFO] Returning streaming response from Ollama._zeroconfai._tcp.local.
[15:53:06] [INFO] Setting up streaming response
INFO:     127.0.0.1:62730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[15:53:06] [DEBUG] Chunk 1: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "Hello"},
[15:53:06] [DEBUG] Chunk 2: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "!"}, "finish_reason": null}]}
[15:53:06] [DEBUG] Chunk 3: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " How"}, "finish_reason": null
[15:53:06] [DEBUG] Chunk 4: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " can"}, "finish_reason": null
[15:53:06] [DEBUG] Chunk 5: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " I"}, "finish_reason": null}]
[15:53:06] [DEBUG] Chunk 6: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " assist"}, "finish_reason": n
[15:53:06] [DEBUG] Chunk 7: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " you"}, "finish_reason": null
[15:53:06] [DEBUG] Chunk 8: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " today"}, "finish_reason": nu
[15:53:07] [DEBUG] Chunk 9: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300387, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "?"}, "finish_reason": null}]}
[15:53:07] [DEBUG] Chunk 10: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300387, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " \ud83d\ude0a"}, "finish_reas
[15:53:07] [DEBUG] Chunk 11: data: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300387, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]}
[15:53:07] [DEBUG] Chunk 12: data: [DONE]
[15:53:07] [INFO] Stream complete. Total chunks: 12
[15:53:15] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:15] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:53:15] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:15] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:53:15] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:15] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:53:15] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:15] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:53:15] [INFO] Ollama._zeroconfai._tcp.local. is now healthy
[15:53:24] [INFO] === NEW REQUEST ===
[15:53:24] [INFO] Model: llama2:latest
[15:53:24] [INFO] Messages: 6
[15:53:24] [INFO] Stream: True
[15:53:24] [INFO] Max tokens: None
[15:53:24] [DEBUG] Message 0: system - You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks
[15:53:24] [DEBUG] Message 1: user - hello
[15:53:24] [DEBUG] Message 2: assistant - .
[15:53:24] [DEBUG] Message 3: user - hi
[15:53:24] [DEBUG] Message 4: assistant - Hello! How can I assist you today? ðŸ˜Š
[15:53:24] [DEBUG] Message 5: user - what is up
[15:53:24] [INFO] Request streaming mode: True
[15:53:24] [DEBUG] Full request data: {
  "model": "llama2:latest",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks to the best of your abilities.\n\nWhen responding:\n- Answer directly from your knowledge when you can\n- Be concise, clear, and helpful\n- Admit when you\u2019re unsure rather than making things up\n\nIf tools are available to you:\n- Only use tools when they add real value to your response\n- Use tools when the user explicitly asks (e.g., \"search for...\", \"calculate...\", \"run this code\")\n- Use tools for information you don\u2019t know or that needs verification\n- Never use tools just because they\u2019re available\n\nWhen using tools:\n- Use one tool at a time and wait for results\n- Use actual values as arguments, not variable names\n- Learn from each result before deciding next steps\n- Avoid repeating the same tool call with identical parameters\n\nRemember: Most questions can be answered without tools. Think first whether you need them.\n\nCurrent date: November 4, 2025"
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "."
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I assist you today? \ud83d\ude0a"
    },
    {
      "role": "user",
      "content": "what is up"
    }
  ],
  "stream": true
}
[15:53:24] [INFO] Routing 'llama2:latest' request to Ollama._zeroconfai._tcp.local. at http://192.168.56.1:8080
[15:53:24] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:24] [DEBUG] http://192.168.56.1:8080 "POST /v1/chat/completions HTTP/1.1" 500 141
[15:53:24] [INFO] Response status: 500
[15:53:24] [DEBUG] Response headers: {'date': 'Tue, 04 Nov 2025 23:53:23 GMT', 'server': 'uvicorn', 'content-length': '141', 'content-type': 'application/json'}
[15:53:24] [ERROR] Exception details: HTTPError: 500 Server Error: Internal Server Error for url: http://192.168.56.1:8080/v1/chat/completions
INFO:     127.0.0.1:62756 - "POST /v1/chat/completions HTTP/1.1" 502 Bad Gateway
[15:53:24] [INFO] === NEW REQUEST ===
[15:53:24] [INFO] Model: llama2:latest
[15:53:24] [INFO] Messages: 6
[15:53:24] [INFO] Stream: True
[15:53:24] [INFO] Max tokens: None
[15:53:24] [DEBUG] Message 0: system - You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks
[15:53:24] [DEBUG] Message 1: user - hello
[15:53:24] [DEBUG] Message 2: assistant - .
[15:53:24] [DEBUG] Message 3: user - hi
[15:53:24] [DEBUG] Message 4: assistant - Hello! How can I assist you today? ðŸ˜Š
[15:53:24] [DEBUG] Message 5: user - what is up
[15:53:24] [INFO] Request streaming mode: True
[15:53:24] [DEBUG] Full request data: {
  "model": "llama2:latest",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks to the best of your abilities.\n\nWhen responding:\n- Answer directly from your knowledge when you can\n- Be concise, clear, and helpful\n- Admit when you\u2019re unsure rather than making things up\n\nIf tools are available to you:\n- Only use tools when they add real value to your response\n- Use tools when the user explicitly asks (e.g., \"search for...\", \"calculate...\", \"run this code\")\n- Use tools for information you don\u2019t know or that needs verification\n- Never use tools just because they\u2019re available\n\nWhen using tools:\n- Use one tool at a time and wait for results\n- Use actual values as arguments, not variable names\n- Learn from each result before deciding next steps\n- Avoid repeating the same tool call with identical parameters\n\nRemember: Most questions can be answered without tools. Think first whether you need them.\n\nCurrent date: November 4, 2025"
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "."
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I assist you today? \ud83d\ude0a"
    },
    {
      "role": "user",
      "content": "what is up"
    }
  ],
  "stream": true
}
[15:53:24] [INFO] Routing 'llama2:latest' request to Ollama._zeroconfai._tcp.local. at http://192.168.56.1:8080
[15:53:24] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:25] [DEBUG] http://192.168.56.1:8080 "POST /v1/chat/completions HTTP/1.1" 500 141
[15:53:25] [INFO] Response status: 500
[15:53:25] [DEBUG] Response headers: {'date': 'Tue, 04 Nov 2025 23:53:23 GMT', 'server': 'uvicorn', 'content-length': '141', 'content-type': 'application/json'}
[15:53:25] [ERROR] Exception details: HTTPError: 500 Server Error: Internal Server Error for url: http://192.168.56.1:8080/v1/chat/completions
INFO:     127.0.0.1:62761 - "POST /v1/chat/completions HTTP/1.1" 502 Bad Gateway
[15:53:25] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:25] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:53:25] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:25] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:53:25] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:25] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:53:25] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:25] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:53:26] [INFO] === NEW REQUEST ===
[15:53:26] [INFO] Model: llama2:latest
[15:53:26] [INFO] Messages: 6
[15:53:26] [INFO] Stream: True
[15:53:26] [INFO] Max tokens: None
[15:53:26] [DEBUG] Message 0: system - You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks
[15:53:26] [DEBUG] Message 1: user - hello
[15:53:26] [DEBUG] Message 2: assistant - .
[15:53:26] [DEBUG] Message 3: user - hi
[15:53:26] [DEBUG] Message 4: assistant - Hello! How can I assist you today? ðŸ˜Š
[15:53:26] [DEBUG] Message 5: user - what is up
[15:53:26] [INFO] Request streaming mode: True
[15:53:26] [DEBUG] Full request data: {
  "model": "llama2:latest",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks to the best of your abilities.\n\nWhen responding:\n- Answer directly from your knowledge when you can\n- Be concise, clear, and helpful\n- Admit when you\u2019re unsure rather than making things up\n\nIf tools are available to you:\n- Only use tools when they add real value to your response\n- Use tools when the user explicitly asks (e.g., \"search for...\", \"calculate...\", \"run this code\")\n- Use tools for information you don\u2019t know or that needs verification\n- Never use tools just because they\u2019re available\n\nWhen using tools:\n- Use one tool at a time and wait for results\n- Use actual values as arguments, not variable names\n- Learn from each result before deciding next steps\n- Avoid repeating the same tool call with identical parameters\n\nRemember: Most questions can be answered without tools. Think first whether you need them.\n\nCurrent date: November 4, 2025"
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "."
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I assist you today? \ud83d\ude0a"
    },
    {
      "role": "user",
      "content": "what is up"
    }
  ],
  "stream": true
}
[15:53:26] [INFO] Routing 'llama2:latest' request to Ollama._zeroconfai._tcp.local. at http://192.168.56.1:8080
[15:53:26] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:26] [DEBUG] http://192.168.56.1:8080 "POST /v1/chat/completions HTTP/1.1" 500 141
[15:53:26] [INFO] Response status: 500
[15:53:26] [DEBUG] Response headers: {'date': 'Tue, 04 Nov 2025 23:53:25 GMT', 'server': 'uvicorn', 'content-length': '141', 'content-type': 'application/json'}
[15:53:26] [ERROR] Exception details: HTTPError: 500 Server Error: Internal Server Error for url: http://192.168.56.1:8080/v1/chat/completions
INFO:     127.0.0.1:62774 - "POST /v1/chat/completions HTTP/1.1" 502 Bad Gateway
[15:53:35] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:35] [DEBUG] http://192.168.56.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:53:35] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8081
[15:53:35] [DEBUG] http://192.168.56.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:53:35] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:35] [DEBUG] http://192.168.56.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:53:35] [DEBUG] Starting new HTTP connection (1): 192.168.56.1:8080
[15:53:35] [DEBUG] http://192.168.56.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:53:35] [INFO] Discovered service: Ollama._zeroconfai._tcp.local. at 169.233.167.164:8080 (priority: 50)
[15:53:35] [INFO] Discovered service: Gemini._zeroconfai._tcp.local. at 172.28.192.1:8081 (priority: 51)
[15:53:35] [INFO] Discovered service: Ollama._zeroconfai._tcp.local. at 172.28.192.1:8080 (priority: 50)
[15:53:35] [INFO] Discovered service: Gemini._zeroconfai._tcp.local. at 172.28.192.1:8081 (priority: 51)
[15:53:35] [INFO] Discovered service: Ollama._zeroconfai._tcp.local. at 172.28.192.1:8080 (priority: 50)
[15:53:35] [INFO] Discovered service: Gemini._zeroconfai._tcp.local. at 172.28.192.1:8081 (priority: 51)
[15:53:45] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8081
[15:53:45] [DEBUG] http://172.28.192.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:53:45] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8081
[15:53:45] [DEBUG] http://172.28.192.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:53:45] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8080
[15:53:45] [DEBUG] http://172.28.192.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:53:45] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8080
[15:53:45] [DEBUG] http://172.28.192.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:53:49] [INFO] === NEW REQUEST ===
[15:53:49] [INFO] Model: qwen3:4b-instruct
[15:53:49] [INFO] Messages: 6
[15:53:49] [INFO] Stream: True
[15:53:49] [INFO] Max tokens: None
[15:53:49] [DEBUG] Message 0: system - You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks
[15:53:49] [DEBUG] Message 1: user - hello
[15:53:49] [DEBUG] Message 2: assistant - .
[15:53:49] [DEBUG] Message 3: user - hi
[15:53:49] [DEBUG] Message 4: assistant - Hello! How can I assist you today? ðŸ˜Š
[15:53:49] [DEBUG] Message 5: user - wait i can talk to you?
[15:53:49] [INFO] Request streaming mode: True
[15:53:49] [DEBUG] Full request data: {
  "model": "qwen3:4b-instruct",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Your primary goal is to assist users with their questions and tasks to the best of your abilities.\n\nWhen responding:\n- Answer directly from your knowledge when you can\n- Be concise, clear, and helpful\n- Admit when you\u2019re unsure rather than making things up\n\nIf tools are available to you:\n- Only use tools when they add real value to your response\n- Use tools when the user explicitly asks (e.g., \"search for...\", \"calculate...\", \"run this code\")\n- Use tools for information you don\u2019t know or that needs verification\n- Never use tools just because they\u2019re available\n\nWhen using tools:\n- Use one tool at a time and wait for results\n- Use actual values as arguments, not variable names\n- Learn from each result before deciding next steps\n- Avoid repeating the same tool call with identical parameters\n\nRemember: Most questions can be answered without tools. Think first whether you need them.\n\nCurrent date: November 4, 2025"
    },
    {
      "role": "user",
      "content": "hello"
    },
    {
      "role": "assistant",
      "content": "."
    },
    {
      "role": "user",
      "content": "hi"
    },
    {
      "role": "assistant",
      "content": "Hello! How can I assist you today? \ud83d\ude0a"
    },
    {
      "role": "user",
      "content": "wait i can talk to you?"
    }
  ],
  "stream": true
}
[15:53:49] [INFO] Routing 'qwen3:4b-instruct' request to Ollama._zeroconfai._tcp.local. at http://172.28.192.1:8080
[15:53:49] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8080
[15:53:55] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8081
[15:53:55] [DEBUG] http://172.28.192.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:53:55] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8081
[15:53:55] [DEBUG] http://172.28.192.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:53:55] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8080
[15:53:58] [INFO] Ollama._zeroconfai._tcp.local. is now unhealthy
[15:54:02] [DEBUG] http://172.28.192.1:8080 "POST /v1/chat/completions HTTP/1.1" 200 None
[15:54:02] [INFO] Response status: 200
[15:54:02] [DEBUG] Response headers: {'date': 'Tue, 04 Nov 2025 23:53:48 GMT', 'server': 'uvicorn', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'x-accel-buffering': 'no', 'content-type': 'text/event-stream; charset=utf-8', 'Transfer-Encoding': 'chunked'}
[15:54:02] [INFO] Returning streaming response from Ollama._zeroconfai._tcp.local.
[15:54:02] [INFO] Setting up streaming response
INFO:     127.0.0.1:62809 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[15:54:02] [DEBUG] Chunk 1: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "Yes"}, "
[15:54:02] [DEBUG] Chunk 2: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": ","}, "finish_reason": null}]}
[15:54:02] [DEBUG] Chunk 3: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " I"}, "finish_reason": null}]
[15:54:02] [DEBUG] Chunk 4: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " can"}, "finish_reason": null
[15:54:02] [DEBUG] Chunk 5: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " talk"}, "finish_reason": nul
[15:54:02] [DEBUG] Chunk 6: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " to"}, "finish_reason": null}
[15:54:02] [DEBUG] Chunk 7: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " you"}, "finish_reason": null
[15:54:02] [DEBUG] Chunk 8: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "!"}, "finish_reason": null}]}
[15:54:02] [DEBUG] Chunk 9: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " \ud83d\ude0a"}, "finish_reas
[15:54:02] [DEBUG] Chunk 10: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " I"}, "finish_reason": null}]
[15:54:02] [DEBUG] Chunk 11: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "'m"}, "finish_reason": null}]
[15:54:02] [DEBUG] Chunk 12: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " here"}, "finish_reason": nul
[15:54:03] [DEBUG] Chunk 13: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " to"}, "finish_reason": null}
[15:54:03] [DEBUG] Chunk 14: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " help"}, "finish_reason": nul
[15:54:03] [DEBUG] Chunk 15: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " with"}, "finish_reason": nul
[15:54:03] [DEBUG] Chunk 16: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " any"}, "finish_reason": null
[15:54:03] [DEBUG] Chunk 17: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " questions"}, "finish_reason"
[15:54:03] [DEBUG] Chunk 18: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " or"}, "finish_reason": null}
[15:54:03] [DEBUG] Chunk 19: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " conversations"}, "finish_rea
[15:54:03] [DEBUG] Chunk 20: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " you"}, "finish_reason": null
[15:54:03] [DEBUG] Chunk 21: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "'d"}, "finish_reason": null}]
[15:54:03] [DEBUG] Chunk 22: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " like"}, "finish_reason": nul
[15:54:04] [DEBUG] Chunk 23: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " to"}, "finish_reason": null}
[15:54:04] [DEBUG] Chunk 24: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " have"}, "finish_reason": nul
[15:54:04] [DEBUG] Chunk 25: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "."}, "finish_reason": null}]}
[15:54:04] [DEBUG] Chunk 26: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " What"}, "finish_reason": nul
[15:54:04] [DEBUG] Chunk 27: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " would"}, "finish_reason": nu
[15:54:04] [DEBUG] Chunk 28: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " you"}, "finish_reason": null
[15:54:04] [DEBUG] Chunk 29: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " like"}, "finish_reason": nul
[15:54:04] [DEBUG] Chunk 30: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " to"}, "finish_reason": null}
[15:54:04] [DEBUG] Chunk 31: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": " discuss"}, "finish_reason":
[15:54:04] [DEBUG] Chunk 32: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {"content": "?"}, "finish_reason": null}]}
[15:54:04] [DEBUG] Chunk 33: data: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qwen3:4b-instruct", "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]}
[15:54:04] [DEBUG] Chunk 34: data: [DONE]
[15:54:04] [INFO] Stream complete. Total chunks: 34
[15:54:08] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8081
[15:54:08] [DEBUG] http://172.28.192.1:8081 "GET /v1/health HTTP/1.1" 200 42
[15:54:08] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8081
[15:54:08] [DEBUG] http://172.28.192.1:8081 "GET /v1/models HTTP/1.1" 200 276
[15:54:08] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8080
[15:54:08] [DEBUG] http://172.28.192.1:8080 "GET /v1/health HTTP/1.1" 200 35
[15:54:08] [DEBUG] Starting new HTTP connection (1): 172.28.192.1:8080
[15:54:08] [DEBUG] http://172.28.192.1:8080 "GET /v1/models HTTP/1.1" 200 136
[15:54:08] [INFO] Ollama._zeroconfai._tcp.local. is now healthy
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [37544]
[15:54:12] [INFO] Shutting down proxy...
PS C:\Users\jperr\Documents\GitHub\Zeroconf-AI>


OLLAMA LOGS (502 errors are from the llama 2 model):
C:\Users\jperr\Documents\GitHub\Zeroconf-AI>python ollama_server.py --priority 50
Starting Ollama proxy on 0.0.0.0:8080 with priority 50...
Ollama._zeroconfai._tcp.local. has been registered.
INFO:     Started server process [19892]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
INFO:     192.168.56.1:49984 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:49986 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:49998 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:50060 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:50078 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:50080 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:50095 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:50097 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:64760 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62699 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62712 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62714 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62726 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62728 - "GET /v1/models HTTP/1.1" 200 OK
Received request for model: qwen3:4b-instruct
Messages count: 4, stream: True
Sending to Ollama: stream=True
Ollama response status: 200
Returning streaming response
INFO:     192.168.56.1:62731 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
INFO:     192.168.56.1:62747 - "GET /v1/health HTTP/1.1" 200 OK
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300386, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300387, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300386", "object": "chat.completion.chunk", "created": 1762300387, "model": "qw
Stream completed
INFO:     192.168.56.1:62752 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62754 - "GET /v1/models HTTP/1.1" 200 OK
Received request for model: llama2:latest
Messages count: 6, stream: True
Sending to Ollama: stream=True
Ollama response status: 500
Ollama error response: {"error":"model requires more system memory than is currently available unable to load full model on GPU"}
INFO:     192.168.56.1:62757 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
Received request for model: llama2:latest
Messages count: 6, stream: True
Sending to Ollama: stream=True
Ollama response status: 500
Ollama error response: {"error":"model requires more system memory than is currently available unable to load full model on GPU"}
INFO:     192.168.56.1:62762 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
INFO:     192.168.56.1:62767 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62769 - "GET /v1/models HTTP/1.1" 200 OK
Received request for model: llama2:latest
Messages count: 6, stream: True
Sending to Ollama: stream=True
Ollama response status: 500
Ollama error response: {"error":"model requires more system memory than is currently available unable to load full model on GPU"}
INFO:     192.168.56.1:62775 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
INFO:     192.168.56.1:62787 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62789 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:62803 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:62805 - "GET /v1/models HTTP/1.1" 200 OK
Received request for model: qwen3:4b-instruct
Messages count: 6, stream: True
Sending to Ollama: stream=True
Ollama response status: 200
Returning streaming response
INFO:     172.28.192.1:62810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
INFO:     172.28.192.1:62822 - "GET /v1/health HTTP/1.1" 200 OK
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300442, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300443, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Sending chunk: {"id": "chatcmpl-1762300442", "object": "chat.completion.chunk", "created": 1762300444, "model": "qw
Stream completed
INFO:     172.28.192.1:62828 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:62830 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58196 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58198 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58205 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58207 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58213 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58215 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58225 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58227 - "GET /v1/models HTTP/1.1" 200 OK

Gemini Logs with no output in Jan:
PS C:\Users\jperr\Documents\GitHub\Zeroconf-AI> python .\gemini_server.py --priority 51
Starting Gemini proxy on 0.0.0.0:8081 with priority 51...
Gemini._zeroconfai._tcp.local. has been registered.
INFO:     Started server process [11540]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8081 (Press CTRL+C to quit)
INFO:     192.168.56.1:49982 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:49983 - "GET /v1/models HTTP/1.1" 200 OK
Received request for model: Gemini-2.5-Flash
Messages count: 2, stream: True
Sending to OpenRouter with model: google/gemini-2.5-flash, stream: True
OpenRouter response status: 200
Returning streaming response
INFO:     192.168.56.1:49989 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     192.168.56.1:49996 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:49997 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:50072 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:50073 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:50093 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:50094 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:64758 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:64759 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62710 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62711 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62724 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62725 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62745 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62746 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62750 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62751 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62765 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62766 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.56.1:62785 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     192.168.56.1:62786 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:62801 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:62802 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:62820 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:62821 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:62826 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:62827 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58192 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58193 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58201 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58202 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58209 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58210 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     172.28.192.1:58219 - "GET /v1/health HTTP/1.1" 200 OK
INFO:     172.28.192.1:58220 - "GET /v1/models HTTP/1.1" 200 OK